{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metrics to assess performance on ordinal classification task given class prediction\n",
    "   using hyper plane loss techniques \n",
    "\"\"\"\n",
    "\n",
    "# Authors: Bob Vanderheyden <rvanderh@us.ibm.com>\n",
    "#          Ying Xie <yxie2@kennesaw.edu>\n",
    "#         \n",
    "# Contributor: Shayan Shamskolahi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def hpall_mean_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce mean).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 0.7228571\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(2,7,.3,1) # ordering_loss_weight must not be less that 1\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(upper_bound,1)\n",
    "    upper_bound = tf.multiply(upper_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(upper_bound)\n",
    "    upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "    lower_loss = tf.subtract(mean_matrix,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_mean(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    mean_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, (hp_ordering_loss)))\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "   \n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpall_sum_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce sum).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 3.48\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_sum_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,.3,0.1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(lower_bound,1)\n",
    "    upper_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(lower_bound)\n",
    "    upper_bound = tf.add(margin, lower_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],mean_matrix)\n",
    "    lower_loss = tf.subtract(y_pred,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_sum(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    sum_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, hp_ordering_loss))\n",
    "    \n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7228571\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_mean_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 0.7228571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.48\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_sum_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 3.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in Keras (mean loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 781 entries, 0 to 781\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Score                         781 non-null    int32  \n",
      " 1   GDP per capita                781 non-null    float64\n",
      " 2   Social support                781 non-null    float64\n",
      " 3   Healthy life expectancy       781 non-null    float64\n",
      " 4   Freedom to make life choices  781 non-null    float64\n",
      " 5   Generosity                    781 non-null    float64\n",
      " 6   Perceptions of corruption     781 non-null    float64\n",
      "dtypes: float64(6), int32(1)\n",
      "memory usage: 45.8 KB\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "pwd = !pwd\n",
    "df = pd.read_csv('world_happiness_2015_2019.csv')\n",
    "df.Score = df.Score.astype('int32')\n",
    "df.drop(['Year'], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7], dtype=int32), array([  7,  89, 202, 249, 162,  72]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example Keras wrapper for hpall_mean_loss\n",
    "\n",
    "def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "    def ohpl(y_true, y_pred):\n",
    "        return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "    return ohpl\n",
    "\n",
    "loss = get_ohpl_wrapper(2,7,.3,1) # ordering_loss_weight must not be less that 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 523 samples\n",
      "Epoch 1/100\n",
      "523/523 [==============================] - 2s 3ms/sample - loss: 4.4571\n",
      "Epoch 2/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 2.3312\n",
      "Epoch 3/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.9290\n",
      "Epoch 4/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.6830\n",
      "Epoch 5/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.8429\n",
      "Epoch 6/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.7596\n",
      "Epoch 7/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.6694\n",
      "Epoch 8/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.4882\n",
      "Epoch 9/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.6276\n",
      "Epoch 10/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.4809\n",
      "Epoch 11/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.4767\n",
      "Epoch 12/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1166\n",
      "Epoch 13/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.3916\n",
      "Epoch 14/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.3656\n",
      "Epoch 15/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.3671\n",
      "Epoch 16/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1404\n",
      "Epoch 17/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2324\n",
      "Epoch 18/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0904\n",
      "Epoch 19/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0993\n",
      "Epoch 20/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2174\n",
      "Epoch 21/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1713\n",
      "Epoch 22/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0335\n",
      "Epoch 23/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2133\n",
      "Epoch 24/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9846\n",
      "Epoch 25/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0301\n",
      "Epoch 26/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2971\n",
      "Epoch 27/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0008\n",
      "Epoch 28/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9611\n",
      "Epoch 29/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0619\n",
      "Epoch 30/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1082\n",
      "Epoch 31/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8778\n",
      "Epoch 32/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1324\n",
      "Epoch 33/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1365\n",
      "Epoch 34/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9023\n",
      "Epoch 35/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2292\n",
      "Epoch 36/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.1038\n",
      "Epoch 37/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9840\n",
      "Epoch 38/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9052\n",
      "Epoch 39/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0690\n",
      "Epoch 40/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9768\n",
      "Epoch 41/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0523\n",
      "Epoch 42/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9350\n",
      "Epoch 43/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9158\n",
      "Epoch 44/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1494\n",
      "Epoch 45/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9344\n",
      "Epoch 46/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7967\n",
      "Epoch 47/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8379\n",
      "Epoch 48/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8964\n",
      "Epoch 49/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9651\n",
      "Epoch 50/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8417\n",
      "Epoch 51/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8984\n",
      "Epoch 52/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9201\n",
      "Epoch 53/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9098\n",
      "Epoch 54/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0923\n",
      "Epoch 55/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0409\n",
      "Epoch 56/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8428\n",
      "Epoch 57/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9043\n",
      "Epoch 58/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0546\n",
      "Epoch 59/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9970\n",
      "Epoch 60/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9922\n",
      "Epoch 61/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8956\n",
      "Epoch 62/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9225\n",
      "Epoch 63/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9232\n",
      "Epoch 64/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9895\n",
      "Epoch 65/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8040\n",
      "Epoch 66/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8324\n",
      "Epoch 67/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8909\n",
      "Epoch 68/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9473\n",
      "Epoch 69/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9500\n",
      "Epoch 70/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9538\n",
      "Epoch 71/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9002\n",
      "Epoch 72/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9915\n",
      "Epoch 73/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0925\n",
      "Epoch 74/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9306\n",
      "Epoch 75/100\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 0.9019\n",
      "Epoch 76/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9569\n",
      "Epoch 77/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9478\n",
      "Epoch 78/100\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 0.8164\n",
      "Epoch 79/100\n",
      "523/523 [==============================] - 2s 3ms/sample - loss: 1.0669\n",
      "Epoch 80/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9533\n",
      "Epoch 81/100\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 0.8929\n",
      "Epoch 82/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9434\n",
      "Epoch 83/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8941\n",
      "Epoch 84/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9199\n",
      "Epoch 85/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7950\n",
      "Epoch 86/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8734\n",
      "Epoch 87/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.8812\n",
      "Epoch 88/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7927\n",
      "Epoch 89/100\n",
      "523/523 [==============================] - 2s 3ms/sample - loss: 0.8257\n",
      "Epoch 90/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9990\n",
      "Epoch 91/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9794\n",
      "Epoch 92/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9036\n",
      "Epoch 93/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8672\n",
      "Epoch 94/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8586\n",
      "Epoch 95/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8074\n",
      "Epoch 96/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9471\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9217\n",
      "Epoch 98/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9606\n",
      "Epoch 99/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9677\n",
      "Epoch 100/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.0677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0fb0c8190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and compile the model \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(6, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the minimum class\n",
    "min_class = min(y_train.unique())\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix from on hot encoded training labels to use to calculate class centroids\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "onehot = onehot_encoder.fit_transform(y_train.reshape((-1, 1)))\n",
    "onehot_inverse = 1/np.sum((onehot.T), axis=1)\n",
    "new_y_train = onehot.T*onehot_inverse.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the training set\n",
    "pred = model.predict(X_train, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply centroid calculation matrix, new_y_train, by training set scores\n",
    "train_cent = np.matmul(new_y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new data model score\n",
    "new_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the closest centroid\n",
    "rcenter = train_cent.T # create row matrix of centroids\n",
    "y_pred = np.argmin(abs(new_pred - rcenter), axis=1) + min_class      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49612403100775193 0.45348837209302323\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean absolute error and mean zero one error\n",
    "mae = np.mean(abs(y_pred - y_test))\n",
    "mze = np.mean(abs(y_pred - y_test) > 0)   \n",
    "print(mae, mze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 18,  3,  3,  0,  0],\n",
       "       [ 2, 18, 27, 17,  1,  0],\n",
       "       [ 0,  1,  7, 52, 22,  3],\n",
       "       [ 0,  0,  0, 13, 25, 17],\n",
       "       [ 0,  0,  0,  1,  6, 19]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(y_test, y_pred) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
