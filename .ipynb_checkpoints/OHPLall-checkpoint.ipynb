{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metrics to assess performance on ordinal classification task given class prediction\n",
    "   using hyper plane loss techniques \n",
    "\"\"\"\n",
    "\n",
    "# Authors: Bob Vanderheyden <rvanderh@us.ibm.com>\n",
    "#          Ying Xie <yxie2@kennesaw.edu>\n",
    "#         \n",
    "# Contributor: Shayan Shamskolahi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def hpall_mean_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce mean).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 0.7228571\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,.3,0.1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(upper_bound,1)\n",
    "    upper_bound = tf.multiply(upper_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(upper_bound)\n",
    "    upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "    lower_loss = tf.subtract(mean_matrix,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_mean(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    mean_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, (hp_ordering_loss)))\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "   \n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpall_sum_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce sum).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 3.48\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_sum_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,.3,0.1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(lower_bound,1)\n",
    "    upper_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(lower_bound)\n",
    "    upper_bound = tf.add(margin, lower_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],mean_matrix)\n",
    "    lower_loss = tf.subtract(y_pred,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_sum(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    sum_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, hp_ordering_loss))\n",
    "    \n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7228571\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_mean_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 0.7228571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.48\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_sum_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 3.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wrapper for Keras (mean loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 781 entries, 0 to 781\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Score                         781 non-null    int32  \n",
      " 1   GDP per capita                781 non-null    float64\n",
      " 2   Social support                781 non-null    float64\n",
      " 3   Healthy life expectancy       781 non-null    float64\n",
      " 4   Freedom to make life choices  781 non-null    float64\n",
      " 5   Generosity                    781 non-null    float64\n",
      " 6   Perceptions of corruption     781 non-null    float64\n",
      "dtypes: float64(6), int32(1)\n",
      "memory usage: 45.8 KB\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "pwd = !pwd\n",
    "df = pd.read_csv('world_happiness_2015_2019.csv')\n",
    "df.Score = df.Score.astype('int32')\n",
    "df.drop(['Year'], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7], dtype=int32), array([  7,  62, 137, 164, 107,  46]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example Keras wrapper for hpall_mean_loss\n",
    "\n",
    "def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "    def ohpl(y_true, y_pred):\n",
    "        return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "    return ohpl\n",
    "\n",
    "loss = get_ohpl_wrapper(2,7,.3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 523 samples\n",
      "Epoch 1/100\n",
      "523/523 [==============================] - 2s 3ms/sample - loss: 6.7586\n",
      "Epoch 2/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 4.9107\n",
      "Epoch 3/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 2.4614\n",
      "Epoch 4/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 2.1560\n",
      "Epoch 5/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 2.0731\n",
      "Epoch 6/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.6746\n",
      "Epoch 7/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.8352\n",
      "Epoch 8/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.5817\n",
      "Epoch 9/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.8087\n",
      "Epoch 10/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.5281\n",
      "Epoch 11/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.4605\n",
      "Epoch 12/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.6421\n",
      "Epoch 13/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.3754\n",
      "Epoch 14/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2085\n",
      "Epoch 15/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.3026\n",
      "Epoch 16/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2065\n",
      "Epoch 17/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2406\n",
      "Epoch 18/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0818\n",
      "Epoch 19/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2974\n",
      "Epoch 20/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0728\n",
      "Epoch 21/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2318\n",
      "Epoch 22/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0198\n",
      "Epoch 23/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2517\n",
      "Epoch 24/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.3860\n",
      "Epoch 25/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1141\n",
      "Epoch 26/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0468\n",
      "Epoch 27/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2050\n",
      "Epoch 28/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.1047\n",
      "Epoch 29/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2055\n",
      "Epoch 30/100\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.2402\n",
      "Epoch 31/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1299\n",
      "Epoch 32/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0455\n",
      "Epoch 33/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9856\n",
      "Epoch 34/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0785\n",
      "Epoch 35/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9678\n",
      "Epoch 36/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9489\n",
      "Epoch 37/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0553\n",
      "Epoch 38/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0724\n",
      "Epoch 39/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1895\n",
      "Epoch 40/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.1268\n",
      "Epoch 41/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0991\n",
      "Epoch 42/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0586\n",
      "Epoch 43/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9988\n",
      "Epoch 44/100\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8984\n",
      "Epoch 45/100\n",
      "165/523 [========>.....................] - ETA: 0s - loss: 0.8029"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(6, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that the training feature set is named \"trainData\" and the training labels are called\n",
    "# \"trainOrds\" and \"l\" is the lowest label for the data set.\n",
    "trainData = X_train\n",
    "trainOrds = np.array(y_train)\n",
    "l = min(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "# Create matrix from on hot encoded training labels to use to calculate class centroids\n",
    "trainOne = onehot_encoder.fit_transform(trainOrds.reshape((-1, 1)))\n",
    "trainInv = 1/np.sum((trainOne.T), axis=1)\n",
    "# Ytrain = trainOne.T*trainInv.reshape(-1,1) #***\n",
    "Ytrain = trainOne.T*trainInv.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the training set\n",
    "train_out = model.predict(trainData, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply centroid calculation matrix, Ytrain, by training set scores\n",
    "# train_cent = np.matmul(Ytrain, train_out) #***\n",
    "train_cent = np.matmul(Ytrain, train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new data model score; assumes feature set name is \"newData\"\n",
    "new_out = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the closest centroid\n",
    "rcenter = train_cent.T # create row matrix of centroids\n",
    "newPred = np.argmin(abs(new_out-rcenter), axis=1)+l     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5426356589147286 0.49224806201550386\n"
     ]
    }
   ],
   "source": [
    "testMAE = np.mean(abs(newPred-y_test))\n",
    "testMZE = np.mean(abs(newPred-y_test)>0)   \n",
    "print(testMAE, testMZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
